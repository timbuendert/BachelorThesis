{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Exploratory Analysis\n",
    "\n",
    "This notebook conducts an exploratory analysis of the dataset under examination. While the notebook section 2.1 assesses the class distribution of the preliminary labeled dataset, 2.2 evaluates the distribution of document lengths as approximated by the number of tokens. Finally, the processed data from notebook #1 is transformed with the TF-IDF 1 method (thesis section 3.3.2) and projected on two dimensions with a PCA (thesis section 3.4.1) and LDA (thesis section 3.4.2) to assess the distribution of documents and the extent of overlapping classes.\n",
    "\n",
    "The results are presented in the thesis section 4.1.\n",
    "\n",
    "Table of Contents:\n",
    "* [2.1 Initial class distribution of labeled document corpus](#dist)\n",
    "* [2.2 Analysis of varying document lengths](#lengths)\n",
    "* [2.3 Two-dimensional projection of labeled dataset](#2d)\n",
    "    * [2.3.1 PCA](#pca)\n",
    "    * [2.3.2 LDA](#lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading modules\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading datasets\n",
    "\n",
    "# load data (labeled_X)\n",
    "# load labels (labeled_y)\n",
    "# load data_unlabeled (unlabeled_X)\n",
    "\n",
    "text_labels = ['Agreement', 'Amendment', 'Attachment', 'LOI', 'NDA', 'Offer', 'SOW']\n",
    "\n",
    "# define minimum frequency of term in analysis to be included in vectorization\n",
    "def min_df(data, percentage):\n",
    "    min_df_c = len(data) * percentage\n",
    "    return int(round(min_df_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Initial class distribution of labeled document corpus <a id=\"dist\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = dict(sorted(Counter(labels).items())) #aggregate classes\n",
    "c.update({k: v/len(labels) for k, v in c.items()}) #transform absolute in relative values\n",
    "for c, p in percentages.items():\n",
    "    print(text_labels[c], round(p * 100, 1), '%') # print relative share of document type in labeled document corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Analysis of varying document lengths <a id=\"lengths\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of number of tokens per document\n",
    "counts = list()\n",
    "for i in range(len(data)): #include lengths of all labeled documents\n",
    "    counts.append(len(data.iloc[i,0]))\n",
    "for i in range(len(data_unlabeled)): #include lengths of all unlabeled documents\n",
    "    counts.append(len(data_unlabeled.iloc[i,0]))\n",
    "    \n",
    "plt.hist(counts, 500, log = True) # visualize distribution with histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Two-dimensional projection of labeled dataset <a id=\"2d\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TF-Idf 1 representation of text\n",
    "labeled_data = data['TEXT_PROCESSED'] #select text\n",
    "unlabeled_data = data_unlabeled['TEXT_PROCESSED'] #select text\n",
    "min_df_c = min_df(labeled_data, 0.05) #compute minimum frequency for term to be included\n",
    "tfidf = TfidfVectorizer(analyzer= 'word', ngram_range = (1, 2), min_df = min_df_c, max_features = None, norm = 'l2', smooth_idf = True, sublinear_tf =True) # initiate uni- and bigram Tf-Idf vectorizer \n",
    "tfidf_vectorizer_best = tfidf.fit(labeled_data) #fit vectorizer on labeled dataset\n",
    "labeled_tfidf = tfidf_vectorizer_best.transform(labeled_data) #transform labeled dataset\n",
    "unlabled_tfidf = tfidf_vectorizer_best.transform(unlabeled_data) #transform unlabeled dataset\n",
    "features = len(tfidf_vectorizer_best.get_feature_names()) #retrieve number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 PCA <a id=\"pca\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array(labeled_tfidf.todense())\n",
    "chunk_size = 1000 # how many document are fed to IPCA at a time, the divisor of n\n",
    "ipca = IncrementalPCA(n_components=2, whiten = False, batch_size=16) # principal component analysis to project data on two dimensions\n",
    "\n",
    "for i in range(0, d.shape[0]//chunk_size): # split dataset into chunks to ensure better run time\n",
    "    ipca.partial_fit(d[i*chunk_size : (i+1)*chunk_size]) # partially fitting the data on IPCA\n",
    "    print(i) # ensure continuous running\n",
    "\n",
    "# successive transformation of labeled documents according to the trained PCA\n",
    "x_1 = ipca.transform(d[:3000])\n",
    "x_2 = ipca.transform(d[3000:6000])\n",
    "x_3 = ipca.transform(d[6000:9000])\n",
    "x_4 = ipca.transform(d[9000:12000])\n",
    "x_5 = ipca.transform(d[12000:15000])\n",
    "x_6 = ipca.transform(d[15000:18000])\n",
    "x_7 = ipca.transform(d[18000:21000])\n",
    "x_8 = ipca.transform(d[21000:])\n",
    "data_pca = np.concatenate((x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8)) # concatenating all documents to one dataset\n",
    "\n",
    "labels_t = [text_labels[i] for i in labels] # retrieve text labels for the legend\n",
    "\n",
    "# plot distribution of documents\n",
    "plt.figure(figsize = (16,10))\n",
    "with sns.color_palette(\"husl\", 7):\n",
    "    sns.scatterplot(\n",
    "    x = data_pca[:, 0], y= data_pca[:, 1],\n",
    "    hue = labels_t,\n",
    "    legend = 'full')\n",
    "    \n",
    "plt.legend(prop={'size': 16}, markerscale = 2.2)\n",
    "plt.xlabel('Principal Component 1', fontsize=16)\n",
    "plt.ylabel('Principal Component 2', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 LDA <a id=\"lda\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components = 2).fit(labeled_tfidf.todense(), labels) # linear discriminant analysis to project data on two dimensions\n",
    "data_lda = lda.transform(labeled_tfidf.todense())\n",
    "\n",
    "labels_t = [text_labels[i] for i in labels] # retrieve text labels for the legend\n",
    "\n",
    "# plot distribution of documents\n",
    "plt.figure(figsize = (16,10))\n",
    "with sns.color_palette(\"husl\", 7):\n",
    "    sns.scatterplot(\n",
    "    x = data_lda[:, 0], y= data_lda[:, 1],\n",
    "    hue = labels_t,\n",
    "    legend = 'full')\n",
    "    \n",
    "plt.legend(prop={'size': 16}, markerscale = 2.2)\n",
    "plt.xlabel('Component 1', fontsize=16)\n",
    "plt.ylabel('Component 2', fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
