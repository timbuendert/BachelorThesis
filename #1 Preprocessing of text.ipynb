{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Preprocessing of text\n",
    "\n",
    "This notebook presents the preprocessing of the document texts as explained in the thesis section 3.2.\n",
    "\n",
    "Table of Contents:\n",
    "* [1.1 Load documents](#load)\n",
    "* [1.2 Preprocessing of document contents](#preprocessing)\n",
    "* [1.3 Export preprocessed text](#export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading modules\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm import tqdm_notebook\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from operator import itemgetter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import spacy\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for saving and loading objects\n",
    "def save_pickle(objectname, picklename): # e.g. save_pickle(contracts_labeled, 'Pickles/contracts_labeled.pickle')\n",
    "    pickle_out = open(picklename,\"wb\") #.pickle\n",
    "    pickle.dump(objectname, pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(picklename, 'successfully pickled.')\n",
    "\n",
    "def load_pickle(picklename): # e.g. contracts_labeled = load_pickle('Pickles/contracts_labeled.pickle')\n",
    "    pickle_in = open(picklename,\"rb\")\n",
    "    return pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load documents <a id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load contracts_labeled\n",
    "#load contracts_unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Preprocess training and test data <a id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing to return the cleaned text as a list of words as string\n",
    "\n",
    "lem = WordNetLemmatizer() #load lemmatizer\n",
    "eng_vocab = set(words.words()) # load Englisch vocabulary\n",
    "nlp = spacy.load(\"de_core_news_sm\") # load German vocabulary\n",
    "ger_vocab = [x.lower() for x in nlp.vocab.strings] # format German vocabulary\n",
    "\n",
    "def lemmatize(word): #function to lemmatize a word\n",
    "    pos_label = (pos_tag(word_tokenize(word))[0][1][0]).lower() #identify single character pos constant from pos_tag\n",
    "    \n",
    "    if pos_label == 'j': pos_label = 'a'    # 'j' <-> 'a' reassignment for adjectives because 'j' is not in wordnet: 'a' as label for adjectives\n",
    "    \n",
    "    if pos_label in ['r']:  # identify and lemmatize adverbs \n",
    "        try:\n",
    "            return wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name()\n",
    "        except (IndexError, WordNetError):\n",
    "            return word\n",
    "    elif pos_label in ['a', 's', 'v']: # identify and lemmatize (satellite) adjectives and verbs\n",
    "        return lem.lemmatize(word, pos=pos_label)\n",
    "    else:   # lemmatize nouns and everything else\n",
    "        return lem.lemmatize(word)\n",
    "        \n",
    "def text_process(text):\n",
    "    nopunc = [char for char in text if char not in string.punctuation] #remove punctuation\n",
    "    nopunc = ''.join([i for i in nopunc if not i.isdigit()]) #remove digits\n",
    "    nopunc =  [word.lower() for word in nopunc.split() if (word not in stopwords.words('english') and word not in stopwords.words('german'))] #remove stopwords\n",
    "    new_text = [lemmatize(word) for word in nopunc] #lemmatize words\n",
    "    new_text = [i for i in new_text if (i in eng_vocab or i in ger_vocab) and len(i) > 1] #keep only real words and remove single character \"words\"\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing of texts\n",
    "contracts_labeled['TEXT_PROCESSED'] = \"\" #create new column for preprocessed text\n",
    "contracts_labeled['TEXT_PROCESSED'] = [' '.join(text_process(str(text))) for text in tqdm_notebook(contracts_labeled['TEXT'], \"Text Processing Labeled\")] #preprocess and join text\n",
    "\n",
    "contracts_unlabeled['TEXT_PROCESSED'] = \"\" #create new column for preprocessed text\n",
    "contracts_unlabeled['TEXT_PROCESSED'] = [' '.join(text_process(str(text))) for text in tqdm_notebook(contracts_unlabeled['TEXT'], \"Text Processing Unlabeled\")] #preprocess and join text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Export preprocessed text <a id=\"export\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labeled and unlabeled DataFrames with preprocessed text\n",
    "data_labeled_X = pd.DataFrame(contracts_labeled[['TEXT_PROCESSED']]).reset_index(drop=True)\n",
    "data_unlabeled_X = pd.DataFrame(contracts_unlabeled[['TEXT_PROCESSED']]).reset_index(drop=True)\n",
    "\n",
    "# convert categorical document classes into numerical labels\n",
    "one_hot_encoder = preprocessing.LabelEncoder().fit(['Agreement', 'Amendment', 'Attachment', 'LOI', 'NDA', 'Offer', 'SOW'])\n",
    "data_labeled_y = one_hot_encoder.transform(contracts_labeled['CON_TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export datasets as pickles\n",
    "save_pickle(data_labeled_X, 'Pickles/1_labeled_X.pickle')\n",
    "save_pickle(data_unlabeled_X, 'Pickles/1_unlabeled_X.pickle')\n",
    "save_pickle(data_labeled_y, 'Pickles/1_labeled_y.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
