{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: Classification\n",
    "\n",
    "The notebook performs the classification task with the machine learning models presented in the thesis section 3.6. \n",
    "As input, the datasets created by the ensemble feature selection method of notebook #3 were included. Both the default and parameter-tuned models are evaluated.\n",
    "Based on the performances of the different classifiers, an ensemble semi-supervised self-training approach is applied which includes the Random Forest, SVM, XGBoost and Deep Neural Network model. \n",
    "\n",
    "In section 4.1 of this notebook, the data is loaded. Subsequently, the performances of the default classifiers with its tuned counterparts are compared. Based on this, 4.3 defines the classifier functions for the subsequent application in 4.4 where the mentioned self-training method is constructed. Finally, 4.5 classifies the remaining documents and 4.6 exports the labels of the unlabeled corpus.\n",
    "Optionally, the class distribution of the preliminary labeled dataset, newly labeled dataset and entire document corpus can be computed.\n",
    "\n",
    "The results are reported in the thesis section 4.3.2.\n",
    "\n",
    "Table of Contents:\n",
    "* [4.1 Loading data](#load)\n",
    "* [4.2 Evaluating the performances of the different models](#scores)\n",
    "* [4.3 Functions for successive usage of best models for training and predicting ](#models)\n",
    "* [4.4 Ensemble self-training](#training)\n",
    "* [4.5 Labeling of remaining documents](#remaining)\n",
    "* [4.6 Export labels](#export)\n",
    "* [Optional: Exploring class distribution](#dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from collections import Counter\n",
    "from scipy.sparse import vstack\n",
    "import string\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm_notebook\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedStratifiedKFold, cross_val_score, cross_validate, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, concatenate, Dropout\n",
    "from keras.layers import Input, Dense, Embedding, Flatten, Conv1D, MaxPooling1D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import Input\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#define function for saving objects (e.g. save_pickle(contracts_labeled, 'Pickles/contracts_labeled.pickle'))\n",
    "def save_pickle(objectname, picklename):\n",
    "    pickle_out = open(picklename,\"wb\") #.pickle\n",
    "    pickle.dump(objectname, pickle_out)\n",
    "    pickle_out.close()\n",
    "    print(picklename, 'successfully pickled.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Loading data <a id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load labeled_X\n",
    "# load labeled_y\n",
    "# load unlabeled_X\n",
    "    \n",
    "# check for correct lengths\n",
    "print(labeled_X.shape, type(labeled_X))\n",
    "print(len(labeled_y), type(labeled_y))\n",
    "print(unlabeled_X.shape, type(unlabeled_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Evaluating performance of best models <a id=\"scores\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct 5 shuffled and stratified folds for evaluating clssifier performance\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# function to plot the confusion matrix\n",
    "def plot_cm(cm):\n",
    "    df_cm = pd.DataFrame(cm, range(len(set(labeled_y))), range(len(set(labeled_y))))\n",
    "    df_cm.index.name = 'True'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    ax = plt.axes()\n",
    "    sns.set(font_scale=1.4) # for label size\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, ax = ax, fmt='g') # font size\n",
    "    plt.show()\n",
    "    \n",
    "# function to return the name of object\n",
    "def namestr(obj):\n",
    "    return [name for name in globals() if globals()[name] is obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include dummy classifier as baseline model\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "f1_w = list()\n",
    "dummy_cm = np.zeros((7, 7))\n",
    "\n",
    "for train_ix, test_ix in kfold.split(labeled_X, labeled_y):\n",
    "    train_X, test_X = labeled_X[train_ix], labeled_X[test_ix]\n",
    "    train_y, test_y = labeled_y[train_ix], labeled_y[test_ix]\n",
    "    clf_deep = deepcopy(dummy_clf)\n",
    "    clf_new = clf_deep.fit(train_X, train_y)\n",
    "    pred_new = clf_new.predict(test_X)\n",
    "    f1_w.append(f1_score(test_y, pred_new, average = 'weighted', labels=np.unique(test_y)))\n",
    "    d_cm = confusion_matrix(test_y, pred_new)\n",
    "    dummy_cm += d_cm\n",
    "\n",
    "print('F1 weighted:', np.mean(f1_w))\n",
    "print('Single folds:', f1_w)\n",
    "plot_cm(dummy_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial naive Bayes (MNB) model: default and tuned\n",
    "\n",
    "mnb_default = MultinomialNB()\n",
    "mnb_best = MultinomialNB(alpha=0.01, fit_prior=True)\n",
    "\n",
    "mnb = [mnb_default, mnb_best]\n",
    "\n",
    "for i in range(len(mnb)): # evaluate default and optimized classifier \n",
    "    print(namestr(mnb[i]))\n",
    "    f1_w = list()\n",
    "    naive_bayes_cm = np.zeros((7, 7))\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(labeled_X, labeled_y):\n",
    "        train_X, test_X = labeled_X[train_ix], labeled_X[test_ix]\n",
    "        train_y, test_y = labeled_y[train_ix], labeled_y[test_ix]\n",
    "        clf_deep = deepcopy(mnb[i])\n",
    "        clf_new = clf_deep.fit(train_X, train_y)\n",
    "        pred_new = clf_new.predict(test_X)\n",
    "        f1_w.append(f1_score(test_y, pred_new, average = 'weighted', labels=np.unique(test_y)))\n",
    "        n0_cm = confusion_matrix(test_y, pred_new)\n",
    "        naive_bayes_cm += n0_cm\n",
    "\n",
    "    print('F1 weighted:', np.mean(f1_w))\n",
    "    print(f1_w)\n",
    "    plot_cm(naive_bayes_cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (SVM) model: default and tuned\n",
    "\n",
    "svc_default = SVC(kernel = 'rbf', class_weight='balanced', decision_function_shape='ovo')\n",
    "svc_best = SVC(C=4, class_weight='balanced', decision_function_shape='ovo', gamma='auto', kernel='rbf',                   \n",
    "               max_iter=-1, random_state=42, shrinking=True, tol=0.001)\n",
    "\n",
    "svc = [svc_default, svc_best]\n",
    "\n",
    "for i in range(len(svc)): # evaluate default and optimized classifier \n",
    "    print(namestr(svc[i]))\n",
    "    f1_w = list()\n",
    "    svc_cm = np.zeros((7, 7))\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(labeled_X, labeled_y):\n",
    "        train_X, test_X = labeled_X[train_ix], labeled_X[test_ix]\n",
    "        train_y, test_y = labeled_y[train_ix], labeled_y[test_ix]\n",
    "        clf_deep = deepcopy(svc[i])\n",
    "        clf_new = clf_deep.fit(train_X, train_y)\n",
    "        pred_new = clf_new.predict(test_X)\n",
    "        f1_w.append(f1_score(test_y, pred_new, average = 'weighted', labels=np.unique(test_y)))\n",
    "        s_cm = confusion_matrix(test_y, pred_new)\n",
    "        svc_cm += s_cm\n",
    "\n",
    "    print('F1 weighted:', np.mean(f1_w))\n",
    "    print(f1_w)\n",
    "    plot_cm(svc_cm) \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest (RF) model: default and tuned\n",
    "\n",
    "rf_default = RandomForestClassifier(n_jobs=-1, random_state = 42, class_weight = 'balanced')\n",
    "rf_best = RandomForestClassifier(bootstrap=True, class_weight='balanced',criterion='gini', \n",
    "                                 max_depth=90, max_features='auto', max_samples=0.3,                                   \n",
    "                                 min_samples_leaf=2, min_samples_split=10, n_estimators=130,                                   \n",
    "                                 n_jobs=-1, random_state=42)\n",
    "\n",
    "rf = [rf_default, rf_best]\n",
    "\n",
    "for i in range(len(rf)): # evaluate default and optimized classifier \n",
    "    print(namestr(rf[i]))\n",
    "    f1_w = list()\n",
    "    rfc_cm = np.zeros((7, 7))\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(labeled_X, labeled_y):\n",
    "        train_X, test_X = labeled_X[train_ix], labeled_X[test_ix]\n",
    "        train_y, test_y = labeled_y[train_ix], labeled_y[test_ix]\n",
    "        clf_deep = deepcopy(rf[i])\n",
    "        clf_new = clf_deep.fit(train_X, train_y)\n",
    "        pred_new = clf_new.predict(test_X)\n",
    "        f1_w.append(f1_score(test_y, pred_new, average = 'weighted', labels=np.unique(test_y)))\n",
    "        rf_cm = confusion_matrix(test_y, pred_new)\n",
    "        rfc_cm += rf_cm\n",
    "\n",
    "    print('F1 weighted:', np.mean(f1_w))\n",
    "    print(f1_w)\n",
    "    plot_cm(rfc_cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# XGBoost (XGB) model: default and tuned\n",
    "\n",
    "class MyXGBClassifier(XGBClassifier):\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return None\n",
    "\n",
    "xgb_default = MyXGBClassifier(booster = 'gbtree', objective= 'multi:softmax', num_class = 7)\n",
    "xgb_best = MyXGBClassifier(learning_rate=0.3, booster = 'gbtree', objective= 'multi:softmax', num_class = 7, \n",
    "                           max_depth = 100, min_child_weight = 1, gamma = 0.2, scale_pos_weight = 1, reg_alpha = 0.25, \n",
    "                           reg_lambda = 1.5, eval_metric = 'merror', n_estimators=300, random_state = 42, n_jobs = -1)   \n",
    "\n",
    "xgb = [xgb_default, xgb_best]\n",
    "\n",
    "for i in range(len(xgb)): # evaluate default and optimized classifier \n",
    "    print(namestr(xgb[i]))\n",
    "    f1_w = list()\n",
    "    xgb_cm = np.zeros((7, 7))\n",
    "\n",
    "    for train_ix, test_ix in kfold.split(labeled_X, labeled_y):\n",
    "        train_X, test_X = labeled_X[train_ix], labeled_X[test_ix]\n",
    "        train_y, test_y = labeled_y[train_ix], labeled_y[test_ix]\n",
    "        clf_deep = deepcopy(xgb[i])\n",
    "        clf_new = clf_deep.fit(train_X, train_y)\n",
    "        pred_new = clf_new.predict(test_X)\n",
    "        f1_w.append(f1_score(test_y, pred_new, average = 'weighted', labels=np.unique(test_y)))\n",
    "        gb_cm = confusion_matrix(test_y, pred_new)\n",
    "        xgb_cm += gb_cm\n",
    "\n",
    "    print('F1 weighted:', np.mean(f1_w))\n",
    "    print(f1_w)\n",
    "    plot_cm(xgb_cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deep Neural Network (DNN) model: basic and tuned\n",
    "\n",
    "# define function for plotting the training history of neural network    \n",
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True,figsize=(10, 6))\n",
    "    fig.suptitle('Model history')\n",
    "    ax1.plot(history.history['categorical_accuracy'])\n",
    "    ax1.plot(history.history['val_categorical_accuracy'])\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    ax1.set_ylabel('categorical accuracy')\n",
    "\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Basic NN: only input and output layer\n",
    "\n",
    "# function to retrieve the NN architecture \n",
    "def get_default_NNmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(614, kernel_initializer=keras.initializers.he_normal(seed=1), activation='relu', input_dim=614))\n",
    "    model.add(keras.layers.Dense(7, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=4), activation='softmax'))\n",
    "    model.compile(optimizer='adam', # setting adaptive learning rate for gradient descent \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy(), #metrics.Accuracy(),\n",
    "                           keras.metrics.AUC()]) #by default ROC curve\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# evaluate performance\n",
    "print('Basic DNN \\n')\n",
    "f1_w = list()\n",
    "NN_cm = np.zeros((7, 7))\n",
    "\n",
    "for train_ix, test_ix in kfold.split(labeled_X, labeled_y):\n",
    "    train_X, test_X = labeled_X[train_ix], labeled_X[test_ix]\n",
    "    train_y, test_y = labeled_y[train_ix], labeled_y[test_ix]\n",
    "    train_yy = keras.utils.to_categorical(train_y)\n",
    "    \n",
    "    model = get_default_NNmodel() #retrieve neural network architecture\n",
    "    history = model.fit(train_X, train_yy, epochs=500, batch_size=512, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=3, restore_best_weights = True)])\n",
    "    pred_y = model.predict(test_X, batch_size=64, verbose=1)\n",
    "    pred_new = np.argmax(pred_y, axis=1)\n",
    "    \n",
    "    f1_w.append(f1_score(test_y, pred_new, average = 'weighted', labels=np.unique(test_y)))\n",
    "    N_cm = confusion_matrix(test_y, pred_new)\n",
    "    NN_cm += N_cm\n",
    "    plot_history(history)\n",
    "\n",
    "print('F1 weighted:', np.mean(f1_w))\n",
    "print(f1_w)   \n",
    "plot_cm(NN_cm)\n",
    "\n",
    " \n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Tuned DNN: input and output layer with two hidden layers in-between\n",
    "\n",
    "# function to retrieve the DNN architecture \n",
    "def get_NNmodel():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(614, kernel_initializer=keras.initializers.he_normal(seed=1), activation='relu', input_dim=614))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(307, kernel_initializer=keras.initializers.he_normal(seed=2), activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    model.add(keras.layers.Dense(153, kernel_initializer=keras.initializers.he_normal(seed=3), activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(7, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=4), activation='softmax'))\n",
    "    model.compile(optimizer='adam', # setting adaptive learning rate for gradient descent \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[keras.metrics.CategoricalAccuracy(),\n",
    "                           keras.metrics.AUC()])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "    \n",
    "# evaluate performance\n",
    "print('Tuned DNN \\n')\n",
    "f1_w = list()\n",
    "NN_cm = np.zeros((7, 7))\n",
    "\n",
    "for train_ix, test_ix in kfold.split(labeled_X, labeled_y):\n",
    "    train_X, test_X = labeled_X[train_ix], labeled_X[test_ix]\n",
    "    train_y, test_y = labeled_y[train_ix], labeled_y[test_ix]\n",
    "    \n",
    "    # usage of resampling methods\n",
    "    smt = SMOTETomek(random_state=42)\n",
    "    X_res, y_res = smt.fit_resample(train_X, train_y)\n",
    "    train_yy = keras.utils.to_categorical(y_res)\n",
    "    \n",
    "    model = get_NNmodel() #retrieve neural network architecture\n",
    "    history = model.fit(X_res, train_yy, epochs=500, batch_size=512, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=3, restore_best_weights = True)]) # min_delta=0.00001\n",
    "    pred_y = model.predict(test_X, batch_size=64, verbose=1)\n",
    "    pred_new = np.argmax(pred_y, axis=1)\n",
    "    \n",
    "    f1_w.append(f1_score(test_y, pred_new, average = 'weighted', labels=np.unique(test_y)))\n",
    "    N_cm = confusion_matrix(test_y, pred_new)\n",
    "    NN_cm += N_cm\n",
    "    plot_history(history)\n",
    "\n",
    "print('F1 weighted:', np.mean(f1_w))\n",
    "print(f1_w)   \n",
    "plot_cm(NN_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Functions for successive usage of best models for training and predicting <a id=\"models\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for successive usage of best SVM model\n",
    "\n",
    "def svc_model(data_X, data_Y, unlabeled_X):\n",
    "    svc = deepcopy(svc_best)\n",
    "    svc_trained = svc.fit(data_X.todense(), data_Y)\n",
    "    pred_svc = svc_trained.predict(unlabeled_X.todense())\n",
    "    print('SVC completed.')\n",
    "    return pred_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for successive usage of best RF model\n",
    "\n",
    "def rf_model(data_X, data_Y, unlabeled_X):\n",
    "    rf = deepcopy(rf_best)\n",
    "    rf_trained = rf.fit(data_X, data_Y)\n",
    "    pred_rf = rf_trained.predict(unlabeled_X)\n",
    "    print('Random Forest completed.')\n",
    "    return pred_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for successive usage of best XGB model\n",
    "\n",
    "class MyXGBClassifier(XGBClassifier):\n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return None\n",
    "    \n",
    "def xgb_model(data_X, data_Y, unlabeled_X):\n",
    "    xgb = deepcopy(xgb_best)    \n",
    "    xgb_trained = xgb.fit(data_X.todense(), data_Y)\n",
    "    pred_xgb = xgb_trained.predict(unlabeled_X.todense())\n",
    "    print('XGBoost completed.')\n",
    "    return pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for successive usage of best DNN model\n",
    "\n",
    "def NN_model(data_X, data_Y, unlabeled_X):\n",
    "    #create (stratified) training and test set for constructing the neural network\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size=0.15, shuffle = True, stratify = data_Y, random_state=42)\n",
    "    yy_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "    # usage of resampling methods\n",
    "    smt = SMOTETomek(random_state=42)\n",
    "    X_res, y_res = smt.fit_resample(X_train, y_train)\n",
    "    yy_train = keras.utils.to_categorical(y_res)\n",
    "\n",
    "    model = get_NNmodel() #retrieve neural network architecture\n",
    "    history = model.fit(X_res, yy_train, epochs=500, batch_size=512, validation_data=(X_test, yy_test), callbacks=[EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=3, restore_best_weights = True)])\n",
    "    plot_history(history) #plot training history\n",
    "\n",
    "    pred_NN = model.predict(unlabeled_X)\n",
    "    pred_FNN = pred_NN.argmax(axis=1)\n",
    "    print('Neural Network completed.')\n",
    "    return pred_FNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Iterative ensemble self-training <a id=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create copies of datasets for tranformation\n",
    "lab_X = deepcopy(labeled_X)\n",
    "lab_y = deepcopy(labeled_y)\n",
    "unlab_X = deepcopy(unlabeled_X)\n",
    "\n",
    "text_labels = ['Agreement', 'Amendment', 'Attachment', 'LOI', 'NDA', 'Offer', 'SOW']\n",
    "unlabeled_labels = [10] * unlab_X.shape[0] # '10' as placeholder for final labels\n",
    "training_iterations = 1 # initiate counter of training iterations\n",
    "\n",
    "c = 1001 # initiate counter of new assignments\n",
    "fraction_unlabeled = unlab_X.shape[0] / (unlab_X.shape[0]+lab_X.shape[0]) # initiate fraction of unlabeled documents of entire corpus\n",
    "\n",
    "# run while a) > 1000 new assignments & b) unlabeled corpus > 5% of entire corpus\n",
    "while (c > 1000) and (fraction_unlabeled > 0.05):\n",
    "    print('Iteration', training_iterations, '\\n') # print current iteration\n",
    "    c = 0 # reset counter of unambiguous assignments\n",
    "    \n",
    "    # train models on labeled corpus and predict unlabeled documents \n",
    "    pred_rf = rf_model(lab_X, lab_y, unlab_X)\n",
    "    pred_svc = svc_model(lab_X, lab_y, unlab_X)\n",
    "    pred_NN = NN_model(lab_X, lab_y, unlab_X)\n",
    "    pred_xgb = xgb_model(lab_X, lab_y, unlab_X)\n",
    "    \n",
    "    # check for correct number of predictions \n",
    "    print('Number of RF predictions:', len(pred_rf))\n",
    "    print('Number of SVM predictions:', len(pred_svc))\n",
    "    print('Number of DNN predictions:', len(pred_NN))\n",
    "    print('Number of XGB predictions:', len(pred_xgb), '\\n')\n",
    "    \n",
    "    indices = list() # reset index of newly labeled documents\n",
    "    predicted_labels = list()  # reset labels of newly labeled documents\n",
    "\n",
    "    for i in range(len(pred_rf)): # iterate through all unlabeled documents\n",
    "        guess_rf = pred_rf[i] #access random forest prediction for this document\n",
    "        guess_svc = pred_svc[i] #access SVM prediction for this document\n",
    "        guess_NN = pred_NN[i] #access deep neural network prediction for this document\n",
    "        guess_xgb = pred_xgb[i] #access XGBoost prediction for this document\n",
    "        predictions_all = [guess_rf, guess_svc, guess_NN, guess_xgb] # combine all predictions for this document\n",
    "        x = len(set(predictions_all)) #evaluate number of different predictions \n",
    "        if x == 1: # if unanimous assignment\n",
    "            indices.append(i) # add index of this document\n",
    "            predicted_labels.append(predictions_all[0]) #add label of this document\n",
    "            c += 1 #increase counter of unanimous assignments\n",
    "    \n",
    "    # print number and share of unanimous assignments\n",
    "    print('{} unanimous assignments (out of {}) completed -> {} missing. \\n'.format(c, len(pred_rf), len(pred_rf)-c))\n",
    "\n",
    "    #evaluate the distribution of new labels\n",
    "    n_pred = dict(sorted(dict(Counter(predicted_labels)).items())) #aggregate list of new labels\n",
    "    for k, v in n_pred.items():\n",
    "        print('{}: {} assignments ({}%)'.format(text_labels[k], v, round(v/len(predicted_labels) *100, 2))) # print absolute and relative frequency of new assignments\n",
    "    print()\n",
    "        \n",
    "    new_labeled = unlab_X[indices] #access the newly assigned documents\n",
    "    print('Shape of newly assigned documents:', new_labeled.shape) #check the shape of newly assigned documents\n",
    "\n",
    "    #update corpora\n",
    "    lab_X = vstack([lab_X, new_labeled]) # add newly assigned documents to labeled corpus\n",
    "    lab_y = np.append(lab_y, np.array(predicted_labels)) # add new labels to labels collection\n",
    "    p = [i for i in range(unlab_X.shape[0]) if i not in indices] # access indexes which were not newly assigned -> remain unlabeled\n",
    "    unlab_X = unlab_X[p] # update unlabeled corpus\n",
    "    # check corpora for correct lengths\n",
    "    print('Shape of updated labeled corpus', lab_X.shape)\n",
    "    print('Length of updated labels collection', lab_y.shape)\n",
    "    print('Shape of updated unlabeled corpus', unlab_X.shape, '\\n')\n",
    "    \n",
    "    # update label distribution \n",
    "    print('Distribution of labels before:', Counter(unlabeled_labels)) #print distribution of labels before updating\n",
    "    unlabeled_counter = -1\n",
    "    new_labels_counter = 0\n",
    "    for i in range(len(unlabeled_labels)): #iterate through all labels\n",
    "        if unlabeled_labels[i] == 10: # check if document is still unlabeled\n",
    "            unlabeled_counter +=1 # increase unlabeled counter\n",
    "        if unlabeled_counter == indices[new_labels_counter]: # check for correct index to update\n",
    "            unlabeled_labels[i] = predicted_labels[new_labels_counter] # update index to the assigned label\n",
    "            new_labels_counter +=1 # increase counter of new labels\n",
    "        if new_labels_counter == len(indices): #exit loop if labels of all newly assigned documents were updated\n",
    "            break\n",
    "    print('Distribution of labels afterwards:', Counter(unlabeled_labels)) #print distribution of labels after updating\n",
    "       \n",
    "    training_iterations += 1 #update number of training iterations\n",
    "    fraction_unlabeled = unlab_X.shape[0] / (unlab_X.shape[0]+lab_X.shape[0]) #compute share of unlabeled documents of the entire corpus\n",
    "    \n",
    "    print('------------------------------------------------------------------------------ \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 Labeling of remaining documents <a id=\"remaining\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the predictions of the models for the remaining documents\n",
    "pred_rf_unlabeled = pred_rf[p]\n",
    "pred_svc_unlabeled = pred_svc[p]\n",
    "pred_NN_unlabeled = pred_NN[p]\n",
    "pred_xgb_unlabeled = pred_xgb[p]\n",
    "\n",
    "#check for correct length\n",
    "print(len(pred_rf_unlabeled))\n",
    "print(len(pred_svc_unlabeled))\n",
    "print(len(pred_NN_unlabeled))\n",
    "print(len(pred_xgb_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning classes to the remaining documents by majority voting or, in case of a draw, by the prediction of the best performing classifier (guess_svc)\n",
    "\n",
    "amount_pred = list()\n",
    "final_pred = list()\n",
    "counter_31 = 0\n",
    "counter_22 = 0\n",
    "\n",
    "for i in range(len(pred_rf_unlabeled)): #access each remaining unlabeled document and load its predictions from the models\n",
    "    guess_rf = pred_rf_unlabeled[i]\n",
    "    guess_svc = pred_svc_unlabeled[i]\n",
    "    guess_NN = pred_NN_unlabeled[i]\n",
    "    guess_xgb = pred_xgb_unlabeled[i]\n",
    "    predictions_all = [guess_rf, guess_svc, guess_NN, guess_xgb] #combine predictions\n",
    "    \n",
    "    x = len(set(predictions_all)) #assess number of different predictions\n",
    "    amount_pred.append(x) # add number of different predictions to list\n",
    "    \n",
    "    #evaluate final label based on number of predictions\n",
    "    if x == 1:\n",
    "        print('Mistake.') # print 'Mistake' because unanimous assignments should have taken place in the iterative self-training\n",
    "        continue\n",
    "    \n",
    "    if x == 2:\n",
    "        c = Counter(predictions_all)\n",
    "        if c.most_common(1)[0][1] == 3: # if predictions were 3:1\n",
    "            pred_class = c.most_common(1)[0][0]\n",
    "            final_pred.append(pred_class) # assign class by majority voting\n",
    "            counter_31 += 1 #increase counter\n",
    "            continue\n",
    "        if c.most_common(1)[0][1] == 2: #if predictions were 2:2\n",
    "            final_pred.append(guess_svc) # assign class by guess_svc as best-performing model\n",
    "            counter_22 += 1 #increase counter\n",
    "            continue\n",
    "    \n",
    "    if x == 3:\n",
    "        c = Counter(predictions_all)\n",
    "        pred_class = c.most_common(1)[0][0] # assign class by majority voting\n",
    "        final_pred.append(pred_class)\n",
    "        continue\n",
    "        \n",
    "    if x == 4:\n",
    "        final_pred.append(guess_svc) # assign class by guess_svc as best-performing model\n",
    "        continue\n",
    "\n",
    "# print the distribution of number of different predictions\n",
    "print('Number of different predictions (e.g. 3 means that there is a majority class as 4 predictions):\\n{} \\n'.format(Counter(amount_pred)))\n",
    "print('For 2 different predictions: \\n{} times 3:1 and {} times 2:2 \\n'.format(counter_31, counter_22))\n",
    "\n",
    "n_pred = dict(sorted(dict(Counter(final_pred)).items())) # assess the class distribution of remaining documents\n",
    "for k, v in n_pred.items():\n",
    "    print('{}: {} assignments ({}%)'.format(text_labels[k], v, round(v/len(final_pred) *100, 2))) # print distribution of final assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final update of label distribution \n",
    "\n",
    "print('Distribution of labels before:', Counter(unlabeled_labels)) #print distribution of labels before updating\n",
    "print('Number of final predictions (labels):', len(final_pred))\n",
    "\n",
    "new_labels_counter = 0\n",
    "for i in range(len(unlabeled_labels)): #iterate through all labels\n",
    "    if unlabeled_labels[i] == 10: # check if document is still unlabeled\n",
    "        unlabeled_labels[i] = final_pred[new_labels_counter] # update to the assigned label\n",
    "        new_labels_counter += 1 # increase counter of new labels\n",
    "    if new_labels_counter == len(final_pred): #exit loop if labels of all newly assigned documents were updated\n",
    "        break\n",
    "\n",
    "print('Distribution of labels afterwards:', Counter(unlabeled_labels)) #print distribution of labels after updating: no '10' should appear anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 Export final predictions <a id=\"export\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unlabeled_labels)) # check for correct length\n",
    "save_pickle(unlabeled_labels, 'Pickles/3_unlabeled_y.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Exploring class distributions <a id=\"dist\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate class distribution of orginally labeled corpus\n",
    "\n",
    "print(len(labeled_y)) #print number of documents in orginally labeled corpus\n",
    "final_counter = dict(Counter(list(labeled_y))) #aggregate document types of corpus\n",
    "print(final_counter) #print aggregated class distribution\n",
    "print()\n",
    "\n",
    "for key in sorted(final_counter): #iterate through document types\n",
    "    # print absolute and relative frequency of this document type\n",
    "    print('{}: {} assignments ({} %)'.format(text_labels[key], final_counter[key], round(final_counter[key]/len(labeled_y) *100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate class distribution of orginally unlabeled corpus\n",
    "\n",
    "print(len(unlabeled_labels)) #print number of documents in orginally unlabeled corpus\n",
    "final_counter = dict(Counter(unlabeled_labels)) #aggregate document types of corpus\n",
    "print(final_counter) #print aggregated class distribution\n",
    "print()\n",
    "\n",
    "for key in sorted(final_counter): #iterate through document types\n",
    "    # print absolute and relative frequency of this document type\n",
    "    print('{}: {} assignments ({} %)'.format(text_labels[key], final_counter[key], round(final_counter[key]/len(unlabeled_labels) *100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate class distribution of entire corpus (originally labeled + unlabeled)\n",
    "both_labels = list(labeled_y) #convert labels to list for concatenation\n",
    "both_labels.extend(unlabeled_labels) # add newly assigned labels to orginal labels -> labels of entire corpus \n",
    "\n",
    "print(len(both_labels)) #print number of documents in entire corpus\n",
    "final_counter = dict(Counter(both_labels)) #aggregate document types of corpus\n",
    "print(final_counter) #print aggregated class distribution\n",
    "print()\n",
    "\n",
    "for key in sorted(final_counter): #iterate through document types\n",
    "    # print absolute and relative frequency of this document type\n",
    "    print('{}: {} assignments ({} %)'.format(text_labels[key], final_counter[key], round(final_counter[key]/len(both_labels) *100, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
